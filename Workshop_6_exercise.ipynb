{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAXIiIhNY2ni"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Wlv/7CS033/Reviews.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "sxeKmAt5wtPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "G6wVoTAp-SKl"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting a sample\n",
        "df = df.sample(n=10000, random_state = 48)\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "Ji0awFAKKRV5"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "%matplotlib notebook\n",
        "# Product Scores\n",
        "fig = px.histogram(df, x=\"Score\")\n",
        "fig.update_traces(marker_color=\"turquoise\",marker_line_color='rgb(8,48,107)',\n",
        "                  marker_line_width=1.5)\n",
        "fig.update_layout(title_text='Product Score')\n",
        "fig.show(renderer=\"colab\")\n"
      ],
      "metadata": {
        "id": "0Yx3fOjNZrZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "# Create stopword list:\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"br\", \"href\"])\n",
        "textt = \" \".join(review for review in df.Text)\n",
        "wordcloud = WordCloud(stopwords=stopwords).generate(textt)\n",
        "%matplotlib inline\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "# plt.savefig('wordcloud11.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BTPAt-aIZsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign reviews with score > 3 as positive sentiment\n",
        "# score < 3 negative sentiment\n",
        "# remove score = 3\n",
        "df = df[df['Score'] != 3]\n",
        "df['sentiment'] = df['Score'].apply(lambda rating : +1 if rating > 3 else -1)"
      ],
      "metadata": {
        "id": "9KYZ-FOSZwam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "gEo4UQQZZ2qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split df - positive and negative sentiment:\n",
        "positive = df[df['sentiment'] == 1]\n",
        "negative = df[df['sentiment'] == -1]"
      ],
      "metadata": {
        "id": "nz0nKUYOZ3hW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word cloud positive\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"br\", \"href\",\"good\",\"great\"])\n",
        "## good and great removed because they were included in negative sentiment\n",
        "pos = \" \".join(review for review in positive.Summary)\n",
        "wordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\n",
        "plt.imshow(wordcloud2, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HnHMSl5jZ8Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word cloud negative\n",
        "\n",
        "neg = \" \".join(str(review) for review in negative.Summary)\n",
        "wordcloud3 = WordCloud(stopwords=stopwords).generate(neg)\n",
        "plt.imshow(wordcloud3, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('wordcloud33.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4WkJlsn6aETG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# review distribution\n",
        "df['sentimentt'] = df['sentiment'].replace({-1 : 'negative'})\n",
        "df['sentimentt'] = df['sentimentt'].replace({1 : 'positive'})\n",
        "fig = px.histogram(df, x=\"sentimentt\")\n",
        "fig.update_traces(marker_color=\"indianred\",marker_line_color='rgb(8,48,107)',\n",
        "                  marker_line_width=1.5)\n",
        "fig.update_layout(title_text='Product Sentiment')\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "bKWJVH33aYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing punctuation method 1\n",
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"'))\n",
        "    return final\n",
        "df['Text'] = df['Text'].apply(remove_punctuation)\n",
        "df = df.dropna(subset=['Summary'])\n",
        "df['Summary'] = df['Summary'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "hMVE3HQfaiDg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation method 2\n",
        "import string\n",
        "string.punctuation\n",
        "df['Text']=df['Text'].apply(lambda x:''.join(i for i in x if i not in string.punctuation))\n",
        "df['Summary']=df['Summary'].apply(lambda x:''.join(i for i in x if i not in string.punctuation))"
      ],
      "metadata": {
        "id": "d8cfLp4099qU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "allstopwords = stopwords.words('english')\n",
        "df['Text']=df.Text.apply(lambda x: \" \".join(i for i in x.split() if i not in allstopwords))\n",
        "df['Summary']=df.Summary.apply(lambda x: \" \".join(i for i in x.split() if i not in allstopwords))"
      ],
      "metadata": {
        "id": "d-5XH7aVIaaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting input and output\n",
        "X=df['Summary']\n",
        "# X=df['Text']\n",
        "y=df['sentiment']"
      ],
      "metadata": {
        "id": "1ncsnsMfauBb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count vectorizer:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "X = vectorizer.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "JSr0mTnza4FH"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "1wygZwoECQpg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: defining the classification models\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "SVM = svm.SVC()\n",
        "RF = RandomForestClassifier()\n",
        "KNN = KNeighborsClassifier()\n",
        "DT=DecisionTreeClassifier()\n",
        "NB = GaussianNB()\n",
        "LR = LogisticRegression()"
      ],
      "metadata": {
        "id": "35ixoccuDWw_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: training the models\n",
        "SVM.fit(X_train, y_train)\n",
        "RF.fit(X_train, y_train)\n",
        "KNN.fit(X_train, y_train)\n",
        "DT.fit(X_train, y_train)\n",
        "LR.fit(X_train,y_train)\n",
        "NB.fit(X_train.toarray(),y_train)"
      ],
      "metadata": {
        "id": "IWQ4AIBcDefl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: prediction\n",
        "y_pred1=SVM.predict(X_test)\n",
        "y_pred2=RF.predict(X_test)\n",
        "y_pred3=KNN.predict(X_test)\n",
        "y_pred4=DT.predict(X_test)\n",
        "y_pred5=LR.predict(X_test)\n",
        "y_pred6=NB.predict(X_test.toarray())"
      ],
      "metadata": {
        "id": "_4fRSg2cEXSU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the confusion matrix (cm) from the cell above and produces all evaluation matrix\n",
        "def confusion_metrics (conf_matrix):\n",
        "\n",
        "    TP = conf_matrix[1][1]\n",
        "    TN = conf_matrix[0][0]\n",
        "    FP = conf_matrix[0][1]\n",
        "    FN = conf_matrix[1][0]\n",
        "    print('True Positives:', TP)\n",
        "    print('True Negatives:', TN)\n",
        "    print('False Positives:', FP)\n",
        "    print('False Negatives:', FN)\n",
        "\n",
        "    # calculate accuracy\n",
        "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
        "\n",
        "    # calculate mis-classification\n",
        "    conf_misclassification = 1- conf_accuracy\n",
        "\n",
        "    # calculate the sensitivity\n",
        "    conf_sensitivity = (TP / float(TP + FN))\n",
        "    # calculate the specificity\n",
        "    conf_specificity = (TN / float(TN + FP))\n",
        "\n",
        "    # calculate precision\n",
        "    conf_precision = (TN / float(TN + FP))\n",
        "    # calculate f_1 score\n",
        "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
        "    print('-'*50)\n",
        "    print(f'Accuracy: {round(conf_accuracy,2)}')\n",
        "    print(f'Mis-Classification: {round(conf_misclassification,2)}')\n",
        "    print(f'Sensitivity: {round(conf_sensitivity,2)}')\n",
        "    print(f'Specificity: {round(conf_specificity,2)}')\n",
        "    print(f'Precision: {round(conf_precision,2)}')\n",
        "    print(f'f_1 Score: {round(conf_f1,2)}')"
      ],
      "metadata": {
        "id": "Xpw2eF_gEYH8"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the confusion matrics for all classifiers' predictions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm1 = confusion_matrix(y_test, y_pred1, labels=SVM.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=SVM.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"SVM\")\n",
        "\n",
        "cm2 = confusion_matrix(y_test, y_pred2, labels=RF.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm2,display_labels=RF.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"RF\")\n",
        "\n",
        "\n",
        "cm3 = confusion_matrix(y_test, y_pred3, labels=KNN.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm3,display_labels=KNN.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"KNN\")\n",
        "\n",
        "cm4 = confusion_matrix(y_test, y_pred4, labels=DT.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm4,display_labels=DT.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"DT\")\n",
        "\n",
        "cm5 = confusion_matrix(y_test, y_pred5, labels=DT.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm5,display_labels=DT.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"LR\")\n",
        "\n",
        "cm6 = confusion_matrix(y_test, y_pred6, labels=DT.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm6,display_labels=DT.classes_)\n",
        "disp.plot()\n",
        "plt.title(\"NB\")"
      ],
      "metadata": {
        "id": "MqJHk7MoG91R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the evaluation metrics for all classifiers\n",
        "print('SVM metrics\\n')\n",
        "confusion_metrics(cm1)\n",
        "print('\\n\\n')\n",
        "print('RF metrics\\n')\n",
        "confusion_metrics(cm2)\n",
        "print('\\n\\n')\n",
        "print('KNN metrics\\n')\n",
        "confusion_metrics(cm3)\n",
        "print('\\n\\n')\n",
        "print('DT metrics\\n')\n",
        "confusion_metrics(cm4)\n",
        "print('\\n\\n')\n",
        "print('LR metrics\\n')\n",
        "confusion_metrics(cm5)\n",
        "print('\\n\\n')\n",
        "print('NB metrics\\n')\n",
        "confusion_metrics(cm6)\n",
        "print('\\n\\n')"
      ],
      "metadata": {
        "id": "WtqOmXe7EfAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}